{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_embedding_naive:  \n",
      " tensor([[[-2.8071,  5.8215, -5.6974,  8.9031, 14.3631,  6.9120,  0.8625,\n",
      "           8.4833],\n",
      "         [-8.9508, -0.5123,  0.2092, -2.2515,  2.1214, -7.5573,  7.4714,\n",
      "           4.2689],\n",
      "         [-0.4929, -2.6356, -0.8143, -5.2192,  6.1698, -2.7434, -4.6646,\n",
      "           9.7776],\n",
      "         [-6.6631, -3.6958, -9.9150, -4.3306,  5.2153,  9.9827, -0.1758,\n",
      "           9.5918]]])\n",
      "The shape of patch_embedding_naive is:  torch.Size([1, 4, 8]) \n",
      "\n",
      "patch_embedding_conv:  \n",
      " tensor([[[-2.8071,  5.8215, -5.6974,  8.9031, 14.3631,  6.9120,  0.8625,\n",
      "           8.4833],\n",
      "         [-8.9508, -0.5123,  0.2092, -2.2515,  2.1214, -7.5573,  7.4714,\n",
      "           4.2689],\n",
      "         [-0.4929, -2.6356, -0.8143, -5.2192,  6.1698, -2.7433, -4.6646,\n",
      "           9.7776],\n",
      "         [-6.6631, -3.6958, -9.9150, -4.3306,  5.2153,  9.9827, -0.1758,\n",
      "           9.5918]]])\n",
      "The shape of patch_embedding_conv is:  torch.Size([1, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "######################### Step1: Convert image to embedding vector sequence #########################\n",
    "def image2emb_naive(image, patch_size, weight):\n",
    "    # image shape: batch_size * channel * h * w\n",
    "    patch = F.unfold(image, kernel_size=patch_size, stride=patch_size).transpose(-2, -1)\n",
    "    # print(\"patch = \", patch)\n",
    "    # print(\"The shape of patch is: \", patch.shape)\n",
    "\n",
    "    patch_embedding = patch @ weight\n",
    "    return patch_embedding\n",
    "\n",
    "def image2emb_conv(image, kernel, stride):\n",
    "    # 可以将image to embedding看做是一个二维卷积，卷积的结果是batch_size*通道数*高度*宽度(因为这里是要模仿NLP，将图片变成一个序列)\n",
    "    conv_output = F.conv2d(image, kernel, stride=stride) # batch_size = output_channel * output_height * output_weight\n",
    "    bs, oc, oh, ow = conv_output.shape\n",
    "    patch_embedding = conv_output.reshape((bs, oc, oh*ow)).transpose(-2, -1)\n",
    "\n",
    "    return patch_embedding\n",
    "\n",
    "# test code for image2emb\n",
    "bs, ic, image_h, image_w = 1, 3, 8, 8 # bs: batch_size; ic: input_channel\n",
    "patch_size = 4\n",
    "model_dim = 8\n",
    "patch_depth = patch_size * patch_size * ic\n",
    "image = torch.randn(bs, ic, image_h, image_w)\n",
    "# print(\"image : \", image)\n",
    "# print(\"The shape of image is: \", image.shape)\n",
    "\n",
    "weight = torch.randn(patch_depth, model_dim) # model_dim是输出通道数目，patch_depth是卷积核的面积乘以输入通道数\n",
    "# print(\"weight = \", weight)\n",
    "# print(\"The shape of weight is: \", weight.shape) # torch.Size([1, 4, 48])  '1': batch_size; '4': patch数目，序列长度。一个8*8的图片，如果以4*4为一块的一共有4块(4就是图片分块过后块的数目); '48':一个patch所包含的像素点的数目。batch_size*batch_size*input_channel(4*4*3)\n",
    "\n",
    "patch_embedding_naive = image2emb_naive(image, patch_size, weight) # 分块方法得到的patch embedding\n",
    "# print(\"patch_embedding_naive = \", patch_embedding_naive, '\\n')\n",
    "# print(\"The shape of patch_embedding_naive is: \", patch_embedding_naive.shape) # torch.Size([1, 4, 8]) 一个图片被分成了4块，每一块用一个长度为8的向量来表示这个块\n",
    "\n",
    "kernel = weight.transpose(0, 1).reshape((-1, ic, patch_size, patch_size))  # output_channel * input_channel * kernel_height * kernel_width\n",
    "patch_embedding_conv = image2emb_conv(image, kernel, patch_size) # 二维卷积方法得到的patch embedding\n",
    "# print(\"patch_embedding_naive: \", '\\n',patch_embedding_naive)\n",
    "# print(\"The shape of patch_embedding_naive is: \", patch_embedding_naive.shape, '\\n')\n",
    "# print(\"patch_embedding_conv: \", '\\n',patch_embedding_conv)\n",
    "# print(\"The shape of patch_embedding_conv is: \", patch_embedding_conv.shape)\n",
    "\n",
    "######################### Step2: CLS token embedding (like BERT) #########################\n",
    "cls_token_embedding = torch.randn(batch_size, 1, model_dim, requires_grad=True)\n",
    "token_embedding = torch.cat([cls_token_embedding, patch_embedding_conv], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before view:  \n",
      " tensor([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,\n",
      "         12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.,\n",
      "         24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,\n",
      "         36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,\n",
      "         48.,  49.,  50.,  51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,  59.,\n",
      "         60.,  61.,  62.,  63.,  64.,  65.,  66.,  67.,  68.,  69.,  70.,  71.,\n",
      "         72.,  73.,  74.,  75.,  76.,  77.,  78.,  79.,  80.,  81.,  82.,  83.,\n",
      "         84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,\n",
      "         96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105., 106., 107.,\n",
      "        108., 109., 110., 111., 112., 113., 114., 115., 116., 117., 118., 119.,\n",
      "        120., 121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,\n",
      "        132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142., 143.,\n",
      "        144., 145., 146., 147., 148., 149., 150., 151., 152., 153., 154., 155.,\n",
      "        156., 157., 158., 159., 160., 161., 162., 163., 164., 165., 166., 167.,\n",
      "        168., 169., 170., 171., 172., 173., 174., 175., 176., 177., 178., 179.,\n",
      "        180., 181., 182., 183., 184., 185., 186., 187., 188., 189., 190., 191.,\n",
      "        192., 193., 194., 195., 196., 197., 198., 199., 200., 201., 202., 203.,\n",
      "        204., 205., 206., 207., 208., 209., 210., 211., 212., 213., 214., 215.,\n",
      "        216., 217., 218., 219., 220., 221., 222., 223., 224., 225., 226., 227.,\n",
      "        228., 229., 230., 231., 232., 233., 234., 235., 236., 237., 238., 239.,\n",
      "        240., 241., 242., 243., 244., 245., 246., 247., 248., 249., 250., 251.,\n",
      "        252., 253., 254., 255., 256., 257., 258., 259., 260., 261., 262., 263.,\n",
      "        264., 265., 266., 267., 268., 269., 270., 271., 272., 273., 274., 275.,\n",
      "        276., 277., 278., 279., 280., 281., 282., 283., 284., 285., 286., 287.,\n",
      "        288., 289., 290., 291., 292., 293., 294., 295., 296., 297., 298., 299.,\n",
      "        300., 301., 302., 303., 304., 305., 306., 307., 308., 309., 310., 311.,\n",
      "        312., 313., 314., 315., 316., 317., 318., 319., 320., 321., 322., 323.,\n",
      "        324., 325., 326., 327., 328., 329., 330., 331., 332., 333., 334., 335.,\n",
      "        336., 337., 338., 339., 340., 341., 342., 343., 344., 345., 346., 347.,\n",
      "        348., 349., 350., 351., 352., 353., 354., 355., 356., 357., 358., 359.,\n",
      "        360., 361., 362., 363., 364., 365., 366., 367., 368., 369., 370., 371.,\n",
      "        372., 373., 374., 375., 376., 377., 378., 379., 380., 381., 382., 383.,\n",
      "        384., 385., 386., 387., 388., 389., 390., 391., 392., 393., 394., 395.,\n",
      "        396., 397., 398., 399., 400., 401., 402., 403., 404., 405., 406., 407.,\n",
      "        408., 409., 410., 411., 412., 413., 414., 415., 416., 417., 418., 419.,\n",
      "        420., 421., 422., 423., 424., 425., 426., 427., 428., 429., 430., 431.,\n",
      "        432., 433., 434., 435., 436., 437., 438., 439., 440., 441., 442., 443.,\n",
      "        444., 445., 446., 447., 448., 449., 450., 451., 452., 453., 454., 455.,\n",
      "        456., 457., 458., 459., 460., 461., 462., 463., 464., 465., 466., 467.,\n",
      "        468., 469., 470., 471., 472., 473., 474., 475., 476., 477., 478., 479.,\n",
      "        480., 481., 482., 483., 484., 485., 486., 487., 488., 489., 490., 491.,\n",
      "        492., 493., 494., 495., 496., 497., 498., 499., 500., 501., 502., 503.,\n",
      "        504., 505., 506., 507., 508., 509., 510., 511., 512., 513., 514., 515.,\n",
      "        516., 517., 518., 519., 520., 521., 522., 523., 524., 525., 526., 527.,\n",
      "        528., 529., 530., 531., 532., 533., 534., 535., 536., 537., 538., 539.,\n",
      "        540., 541., 542., 543., 544., 545., 546., 547., 548., 549., 550., 551.,\n",
      "        552., 553., 554., 555., 556., 557., 558., 559., 560., 561., 562., 563.,\n",
      "        564., 565., 566., 567., 568., 569., 570., 571., 572., 573., 574., 575.,\n",
      "        576., 577., 578., 579., 580., 581., 582., 583., 584., 585., 586., 587.,\n",
      "        588., 589., 590., 591., 592., 593., 594., 595., 596., 597., 598., 599.,\n",
      "        600., 601., 602., 603., 604., 605., 606., 607., 608., 609., 610., 611.,\n",
      "        612., 613., 614., 615., 616., 617., 618., 619., 620., 621., 622., 623.,\n",
      "        624., 625., 626., 627., 628., 629., 630., 631., 632., 633., 634., 635.,\n",
      "        636., 637., 638., 639., 640., 641., 642., 643., 644., 645., 646., 647.,\n",
      "        648., 649., 650., 651., 652., 653., 654., 655., 656., 657., 658., 659.,\n",
      "        660., 661., 662., 663., 664., 665., 666., 667., 668., 669., 670., 671.,\n",
      "        672., 673., 674.])\n",
      "The shape of x is:  torch.Size([675]) \n",
      "\n",
      "After view:  \n",
      " tensor([[[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
      "            11.,  12.,  13.,  14.],\n",
      "          [ 15.,  16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,\n",
      "            26.,  27.,  28.,  29.],\n",
      "          [ 30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,\n",
      "            41.,  42.,  43.,  44.],\n",
      "          [ 45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,  55.,\n",
      "            56.,  57.,  58.,  59.],\n",
      "          [ 60.,  61.,  62.,  63.,  64.,  65.,  66.,  67.,  68.,  69.,  70.,\n",
      "            71.,  72.,  73.,  74.],\n",
      "          [ 75.,  76.,  77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,\n",
      "            86.,  87.,  88.,  89.],\n",
      "          [ 90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,  99., 100.,\n",
      "           101., 102., 103., 104.],\n",
      "          [105., 106., 107., 108., 109., 110., 111., 112., 113., 114., 115.,\n",
      "           116., 117., 118., 119.],\n",
      "          [120., 121., 122., 123., 124., 125., 126., 127., 128., 129., 130.,\n",
      "           131., 132., 133., 134.],\n",
      "          [135., 136., 137., 138., 139., 140., 141., 142., 143., 144., 145.,\n",
      "           146., 147., 148., 149.],\n",
      "          [150., 151., 152., 153., 154., 155., 156., 157., 158., 159., 160.,\n",
      "           161., 162., 163., 164.],\n",
      "          [165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.,\n",
      "           176., 177., 178., 179.],\n",
      "          [180., 181., 182., 183., 184., 185., 186., 187., 188., 189., 190.,\n",
      "           191., 192., 193., 194.],\n",
      "          [195., 196., 197., 198., 199., 200., 201., 202., 203., 204., 205.,\n",
      "           206., 207., 208., 209.],\n",
      "          [210., 211., 212., 213., 214., 215., 216., 217., 218., 219., 220.,\n",
      "           221., 222., 223., 224.]],\n",
      "\n",
      "         [[225., 226., 227., 228., 229., 230., 231., 232., 233., 234., 235.,\n",
      "           236., 237., 238., 239.],\n",
      "          [240., 241., 242., 243., 244., 245., 246., 247., 248., 249., 250.,\n",
      "           251., 252., 253., 254.],\n",
      "          [255., 256., 257., 258., 259., 260., 261., 262., 263., 264., 265.,\n",
      "           266., 267., 268., 269.],\n",
      "          [270., 271., 272., 273., 274., 275., 276., 277., 278., 279., 280.,\n",
      "           281., 282., 283., 284.],\n",
      "          [285., 286., 287., 288., 289., 290., 291., 292., 293., 294., 295.,\n",
      "           296., 297., 298., 299.],\n",
      "          [300., 301., 302., 303., 304., 305., 306., 307., 308., 309., 310.,\n",
      "           311., 312., 313., 314.],\n",
      "          [315., 316., 317., 318., 319., 320., 321., 322., 323., 324., 325.,\n",
      "           326., 327., 328., 329.],\n",
      "          [330., 331., 332., 333., 334., 335., 336., 337., 338., 339., 340.,\n",
      "           341., 342., 343., 344.],\n",
      "          [345., 346., 347., 348., 349., 350., 351., 352., 353., 354., 355.,\n",
      "           356., 357., 358., 359.],\n",
      "          [360., 361., 362., 363., 364., 365., 366., 367., 368., 369., 370.,\n",
      "           371., 372., 373., 374.],\n",
      "          [375., 376., 377., 378., 379., 380., 381., 382., 383., 384., 385.,\n",
      "           386., 387., 388., 389.],\n",
      "          [390., 391., 392., 393., 394., 395., 396., 397., 398., 399., 400.,\n",
      "           401., 402., 403., 404.],\n",
      "          [405., 406., 407., 408., 409., 410., 411., 412., 413., 414., 415.,\n",
      "           416., 417., 418., 419.],\n",
      "          [420., 421., 422., 423., 424., 425., 426., 427., 428., 429., 430.,\n",
      "           431., 432., 433., 434.],\n",
      "          [435., 436., 437., 438., 439., 440., 441., 442., 443., 444., 445.,\n",
      "           446., 447., 448., 449.]],\n",
      "\n",
      "         [[450., 451., 452., 453., 454., 455., 456., 457., 458., 459., 460.,\n",
      "           461., 462., 463., 464.],\n",
      "          [465., 466., 467., 468., 469., 470., 471., 472., 473., 474., 475.,\n",
      "           476., 477., 478., 479.],\n",
      "          [480., 481., 482., 483., 484., 485., 486., 487., 488., 489., 490.,\n",
      "           491., 492., 493., 494.],\n",
      "          [495., 496., 497., 498., 499., 500., 501., 502., 503., 504., 505.,\n",
      "           506., 507., 508., 509.],\n",
      "          [510., 511., 512., 513., 514., 515., 516., 517., 518., 519., 520.,\n",
      "           521., 522., 523., 524.],\n",
      "          [525., 526., 527., 528., 529., 530., 531., 532., 533., 534., 535.,\n",
      "           536., 537., 538., 539.],\n",
      "          [540., 541., 542., 543., 544., 545., 546., 547., 548., 549., 550.,\n",
      "           551., 552., 553., 554.],\n",
      "          [555., 556., 557., 558., 559., 560., 561., 562., 563., 564., 565.,\n",
      "           566., 567., 568., 569.],\n",
      "          [570., 571., 572., 573., 574., 575., 576., 577., 578., 579., 580.,\n",
      "           581., 582., 583., 584.],\n",
      "          [585., 586., 587., 588., 589., 590., 591., 592., 593., 594., 595.,\n",
      "           596., 597., 598., 599.],\n",
      "          [600., 601., 602., 603., 604., 605., 606., 607., 608., 609., 610.,\n",
      "           611., 612., 613., 614.],\n",
      "          [615., 616., 617., 618., 619., 620., 621., 622., 623., 624., 625.,\n",
      "           626., 627., 628., 629.],\n",
      "          [630., 631., 632., 633., 634., 635., 636., 637., 638., 639., 640.,\n",
      "           641., 642., 643., 644.],\n",
      "          [645., 646., 647., 648., 649., 650., 651., 652., 653., 654., 655.,\n",
      "           656., 657., 658., 659.],\n",
      "          [660., 661., 662., 663., 664., 665., 666., 667., 668., 669., 670.,\n",
      "           671., 672., 673., 674.]]]])\n",
      "The shape of x is:  torch.Size([1, 3, 15, 15]) \n",
      "\n",
      "x1 =  tensor([[[  0.,   1.,   2.,  ..., 190., 191., 192.],\n",
      "         [  1.,   2.,   3.,  ..., 191., 192., 193.],\n",
      "         [  2.,   3.,   4.,  ..., 192., 193., 194.],\n",
      "         ...,\n",
      "         [480., 481., 482.,  ..., 670., 671., 672.],\n",
      "         [481., 482., 483.,  ..., 671., 672., 673.],\n",
      "         [482., 483., 484.,  ..., 672., 673., 674.]]])\n",
      "The shape of x1 is:  torch.Size([1, 27, 169])\n"
     ]
    }
   ],
   "source": [
    "# torch.nn.functional.unfold()测试\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.arange(0, 1*3*15*15).float()\n",
    "print(\"Before view: \", '\\n', x)\n",
    "print(\"The shape of x is: \", x.shape, '\\n')\n",
    "\n",
    "x = x.view(1, 3, 15, 15)\n",
    "print(\"After view: \", '\\n',x)\n",
    "print(\"The shape of x is: \", x.shape, '\\n')\n",
    "\n",
    "x1 = F.unfold(x, kernel_size=3, dilation=1, stride=1)\n",
    "print(\"x1 = \", x1)\n",
    "print(\"The shape of x1 is: \", x1.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_torch112_cuda116",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
