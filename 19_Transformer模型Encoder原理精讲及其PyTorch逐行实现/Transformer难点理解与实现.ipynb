{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before padding: \n",
      " [tensor([6, 3]), tensor([3, 2, 1, 7])]\n",
      "After padding:  \n",
      " tensor([[4, 4, 0, 0, 0],\n",
      "        [2, 4, 5, 7, 0]])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Before padding: \n",
      " [tensor([3, 4, 5, 1]), tensor([7, 1, 5])]\n",
      "After padding: \n",
      " tensor([[4, 5, 7, 6, 0],\n",
      "        [5, 1, 3, 0, 0]])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parameter containing:\n",
      "tensor([[ 0.3933, -1.3552,  0.3834, -0.2992, -0.2086,  1.1328, -0.6057, -0.0127],\n",
      "        [ 0.8701, -0.0343, -0.5782, -0.0905,  2.0679, -1.6769, -0.9496,  0.4667],\n",
      "        [ 0.3211,  0.5684, -1.4334,  0.3410, -0.5727,  0.5528,  0.2761, -0.9537],\n",
      "        [-3.3242, -0.6343,  0.8966, -0.1813, -2.4476,  1.9957, -0.8261, -0.8904],\n",
      "        [ 0.0540, -1.0303,  1.0239,  0.5747,  0.3174, -1.3341,  1.1541, -1.6357],\n",
      "        [ 0.1257,  1.6404, -1.4771,  0.6476,  0.6719,  0.5688,  0.9016,  1.1626],\n",
      "        [ 0.9428, -0.2309, -0.4926, -0.8419,  1.0563, -0.9783,  0.7129, -0.0228],\n",
      "        [ 0.5351, -0.5089, -1.3434,  0.0101,  2.1497, -0.2662,  0.9699, -0.1244],\n",
      "        [-0.3931,  0.3205,  1.9094, -0.3123,  0.6467,  0.0308,  1.4656,  1.4895]],\n",
      "       requires_grad=True)\n",
      "The size of src_embedding_table.weight is:  Parameter containing:\n",
      "tensor([[ 0.3933, -1.3552,  0.3834, -0.2992, -0.2086,  1.1328, -0.6057, -0.0127],\n",
      "        [ 0.8701, -0.0343, -0.5782, -0.0905,  2.0679, -1.6769, -0.9496,  0.4667],\n",
      "        [ 0.3211,  0.5684, -1.4334,  0.3410, -0.5727,  0.5528,  0.2761, -0.9537],\n",
      "        [-3.3242, -0.6343,  0.8966, -0.1813, -2.4476,  1.9957, -0.8261, -0.8904],\n",
      "        [ 0.0540, -1.0303,  1.0239,  0.5747,  0.3174, -1.3341,  1.1541, -1.6357],\n",
      "        [ 0.1257,  1.6404, -1.4771,  0.6476,  0.6719,  0.5688,  0.9016,  1.1626],\n",
      "        [ 0.9428, -0.2309, -0.4926, -0.8419,  1.0563, -0.9783,  0.7129, -0.0228],\n",
      "        [ 0.5351, -0.5089, -1.3434,  0.0101,  2.1497, -0.2662,  0.9699, -0.1244],\n",
      "        [-0.3931,  0.3205,  1.9094, -0.3123,  0.6467,  0.0308,  1.4656,  1.4895]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 关于word embedding，以序列建模为例\n",
    "# 考虑source sentence 和 target sentence\n",
    "# 构建序列，序列的字符以其在词表中的索引的形式表示\n",
    "\n",
    "# 先把目标序列和源序列的长度假设一个值，然后再根据这个长度再去随机生成单词的索引\n",
    "batch_size = 2\n",
    "\n",
    "# 单词表大小\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "model_dim = 8 # 原论文中是512\n",
    "\n",
    "# 序列的最大长度\n",
    "max_src_seq_len = 5\n",
    "max_tgt_seq_len = 5\n",
    "\n",
    "#src_len = torch.randint(2, 5, (batch_size, ))\n",
    "#tgt_len = torch.randint(2, 5, (batch_size, ))\n",
    "\n",
    "src_len = torch.Tensor([2, 4]).to(torch.int32)\n",
    "tgt_len = torch.Tensor([4, 3]).to(torch.int32)\n",
    "\n",
    "# 单词索引(Token ID)构成源句子和目标句子，并且做了padding，默认值为0\n",
    "src_seq = [torch.randint(1, max_num_src_words, (L,)) for L in src_len]\n",
    "src_seq_pad = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_src_words, (L,)), (0, max_src_seq_len -L)), 0) for L in src_len], dim=0) \n",
    "tgt_seq = [torch.randint(1, max_num_tgt_words, (L,)) for L in tgt_len]\n",
    "tgt_seq_pad = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_tgt_words, (L,)), (0, max_tgt_seq_len -L)), 0) for L in tgt_len], dim=0)\n",
    "\n",
    "print(\"Before padding:\", '\\n', src_seq)\n",
    "print(\"After padding: \", '\\n', src_seq_pad)\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "print(\"Before padding:\", '\\n', tgt_seq)\n",
    "print(\"After padding:\", '\\n', tgt_seq_pad)\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "\n",
    "# 构造embedding\n",
    "\"\"\"为什么要+1：\n",
    "  在创建嵌入表时，通常需要为每个可能的索引（包括0）提供一个嵌入向量。\n",
    "  这里的 max_num_src_words 表示源句子中可能出现的最大不同单词的数量。\n",
    "  如果我们只创建 max_num_src_words 个嵌入向量，那么我们就没有为索引0提供嵌入向量。\n",
    "  在许多情况下，索引0用于表示特殊的“填充”（padding）标记，这是在处理变长序列时常用的技术。\"\"\"\n",
    "src_embedding_table = nn.Embedding(max_num_src_words+1, model_dim) \n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words+1, model_dim)\n",
    "print(src_embedding_table.weight)\n",
    "print(\"The size of src_embedding_table.weight is: \", src_embedding_table.weight)\n",
    "# https://www.bilibili.com/video/BV1cP4y1V7GF?t=2112.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.4551,  0.3129,  1.1398],\n",
      "         [-2.1301,  0.4180,  0.4204],\n",
      "         [-0.1716, -1.8632,  0.3244],\n",
      "         [-0.5084, -1.6552, -1.8416]],\n",
      "\n",
      "        [[-0.1716, -1.8632,  0.3244],\n",
      "         [ 0.6709,  0.4043, -2.2828],\n",
      "         [-2.1301,  0.4180,  0.4204],\n",
      "         [ 0.3204,  1.6376, -0.4442]]], grad_fn=<EmbeddingBackward0>)\n",
      "The shape of e is:  torch.Size([2, 4, 3])\n",
      "tensor([ 0.3204,  1.6376, -0.4442], grad_fn=<SelectBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parameter containing:\n",
      "tensor([[-1.7047,  0.8644, -0.4895],\n",
      "        [-1.4551,  0.3129,  1.1398],\n",
      "        [-2.1301,  0.4180,  0.4204],\n",
      "        [ 0.6709,  0.4043, -2.2828],\n",
      "        [-0.1716, -1.8632,  0.3244],\n",
      "        [-0.5084, -1.6552, -1.8416],\n",
      "        [-1.5301,  0.2444, -0.6057],\n",
      "        [ 2.0089,  1.4314,  0.9333],\n",
      "        [-2.0367,  0.4912,  0.7898],\n",
      "        [ 0.3204,  1.6376, -0.4442]], requires_grad=True)\n",
      "The shape of embedding.weight is:  torch.Size([10, 3])\n"
     ]
    }
   ],
   "source": [
    "# torch.nn.Embedding()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embedding = nn.Embedding(10, 3) # 定义一个embedding模块，包含了一个长度为10的张量，每个张量的大小是3\n",
    "# print(embedding)\n",
    "input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
    "e = embedding(input)\n",
    "print(e) # e.shape: [2, 4, 3], 在经过nn.embedding后，从[2, 4]维度变换为[2, 4, 3]，其实就是[2, 4]中的每个值作为索引去nn.embedding中取对应的权重\n",
    "print(\"The shape of e is: \", e.shape)\n",
    "print(e[1][3]) # [0.3204,  1.6376, -0.4442]，相应的3即为embedding后的权重\n",
    "print(\"-\"*100)\n",
    "print(embedding.weight) \n",
    "print(\"The shape of embedding.weight is: \", embedding.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 1., 1.]])\n",
      "tensor([[[0., 0., 0., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# encoder self-attention\n",
    "\n",
    "import torch\n",
    "\n",
    "seq_len = 5\n",
    "batch_size = 2\n",
    "attention_mask = torch.zeros((batch_size, seq_len)) # 全0矩阵，表示没有padding\n",
    "# print(attention_mask)\n",
    "\n",
    "attention_mask[:, 3:] = 1 # 将需要padding的位置设置成1\n",
    "# print(attention_mask)\n",
    "\n",
    "extended_attention_mask = attention_mask.unsqueeze(1)\n",
    "\n",
    "print(extended_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_torch112_cuda116",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
